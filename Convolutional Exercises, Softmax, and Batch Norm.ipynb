{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{bmatrix} 3 & 0 & 1 & 2 & 7 & 4 \\\\ 1 & 5 & 8 & 9 & 3 & 1 \\\\ 2 & 7 & 2 & 5 & 1 & 3 \\\\ 0 & 1 & 3 & 1 & 7 & 8 \\\\ 4 & 2 & 1 & 6 & 2 & 8 \\\\ 2 & 4 & 5 & 2 & 3 & 9\\end{bmatrix} \\circledast \\begin{bmatrix} 1 & 0 & -1 \\\\ 1 & 0 & -1 \\\\ 1 & 0 & -1 \\end{bmatrix} = \\begin{bmatrix} ? & ? & ? & ? \\\\ ? & ? & ? & ? \\\\ ? & ? & ? & ? \\\\ ? & ? & ? & ? \\end{bmatrix}$$\n",
    "\n",
    "<p style=\"text-align: center;\"><b>Figure 1</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using Figure 1, find the resulting matrix.\n",
    "\n",
    "\n",
    "2. Perform average pooling to get the resulting matrix from 4x4 to a 2x2 matrix.\n",
    "\n",
    "\n",
    "3. How much padding would we need to add to the input matrix to ensure the resulting matrix is the same size (i.e. 6x6)?\n",
    "\n",
    "\n",
    "4. What is a batch or mini-batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation\n",
      "[[ -5  -4   0   8]\n",
      " [-10  -2   2   3]\n",
      " [  0  -2  -4  -7]\n",
      " [ -3  -2  -3 -16]]\n",
      "\n",
      "Convolution\n",
      "[[ 5  4  0 -8]\n",
      " [10  2 -2 -3]\n",
      " [ 0  2  4  7]\n",
      " [ 3  2  3 16]]\n",
      "\n",
      "Correlation with Padding of 0s\n",
      "[[  2  -3  -8  -7   4   8]\n",
      " [ -6  -5  -4   0   8   3]\n",
      " [-10 -10  -2   2   3  -1]\n",
      " [ -4   0  -2  -4  -7  -9]\n",
      " [ -1  -3  -2  -3 -16 -13]\n",
      " [ -2  -3   0   3 -16 -18]]\n",
      "\n",
      "Convolution with Padding of 0s\n",
      "[[-2  3  8  7 -4 -8]\n",
      " [ 6  5  4  0 -8 -3]\n",
      " [10 10  2 -2 -3  1]\n",
      " [ 4  0  2  4  7  9]\n",
      " [ 1  3  2  3 16 13]\n",
      " [ 2  3  0 -3 16 18]]\n",
      "\n",
      "Average Pooling\n",
      "[[-5.25  3.25]\n",
      " [-1.75 -7.5 ]]\n",
      "\n",
      "Max Pooling\n",
      "[[-2  8]\n",
      " [ 0 -3]]\n",
      "[[10  0]\n",
      " [ 3 16]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import signal, ndimage\n",
    "from skimage import measure\n",
    "\n",
    "input_arr = np.array([[3, 0, 1, 2, 7, 4],\n",
    "                      [1, 5, 8, 9, 3, 1],\n",
    "                      [2, 7, 2, 5, 1, 3],\n",
    "                      [0, 1, 3, 1, 7, 8],\n",
    "                      [4, 2, 1, 6, 2, 8],\n",
    "                      [2, 4, 5, 2, 3, 9]])\n",
    "\n",
    "kerna_arr = np.array([[1, 0, -1],\n",
    "                      [1, 0, -1],\n",
    "                      [1, 0, -1]])\n",
    "\n",
    "#NOTE THE CORRELATE INSTEAD OF CONVOLVE. IN CONVNETS WE DO CORRELATIONS\n",
    "print(\"Correlation\")\n",
    "a_out = signal.correlate(input_arr, kerna_arr, mode='valid')\n",
    "print(a_out)\n",
    "print(\"\\nConvolution\")\n",
    "b_out = signal.convolve(input_arr, kerna_arr, mode='valid')\n",
    "print(b_out)\n",
    "print(\"\\nCorrelation with Padding of 0s\")\n",
    "print(ndimage.correlate(input_arr, kerna_arr))\n",
    "print(\"\\nConvolution with Padding of 0s\")\n",
    "print(ndimage.convolve(input_arr, kerna_arr))\n",
    "\n",
    "print(\"\\nAverage Pooling\")\n",
    "print(measure.block_reduce(a_out, (2,2), np.average))\n",
    "\n",
    "print(\"\\nMax Pooling\")\n",
    "print(measure.block_reduce(a_out, (2,2), np.max))\n",
    "print(measure.block_reduce(b_out, (2,2), np.max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "Think back to logistic regression and the equations we needed to use there. For multinomial logistic regression the goal was to find the probability of the output as each of the classes. This required we have a vector output $z = [z_1, z_2, ..., z_k]$ of $k$ arbitrary values. This vector was then mapped to a probability distribution with each value in the range $(0,1)$ and altogether summing to $1$. This resulted in us deriving the softmax function:\n",
    "\n",
    "$$softmax(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}} 1 \\le i \\le k $$\n",
    "\n",
    "We occassionally need to do this in Neural Networks as well. We have multiple outputs and so in order to effectively get a resulting classification, we apply the softmax so each output node gives us the probability of itself being correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Norm\n",
    "We have already discussed what batches are in regards to neural networks. [Batch normalization](https://en.wikipedia.org/wiki/Batch_normalization) is exactly what it sounds like. Each layer in the network that does batch normalization re-centers and re-scales to a normal distribution so we don't suffer internal covariate shift during training. If you want to look at the equations done to achieve batch normalization at each layer please check out the wiki page. The key part is understanding that batch normalization is just normalizing the input so we don't have sporatic layers with unexpected inputs resulting in covariate shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
