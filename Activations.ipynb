{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Torch has so many activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) and they are constantly adding to it. Below are a couple of the popular ones written out via their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-152.8900]) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "# input = torch.randn(1)\n",
    "# input = torch.tensor([152.89])\n",
    "# input = torch.tensor([250.])\n",
    "input = torch.tensor([-152.89])\n",
    "\n",
    "m = nn.ReLU()\n",
    "output = m(input)\n",
    "print(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-152.8900]) tensor([-15.2890])\n"
     ]
    }
   ],
   "source": [
    "m = nn.LeakyReLU(0.1)\n",
    "output = m(input)\n",
    "print(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-152.8900]) tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Sigmoid()\n",
    "output = m(input)\n",
    "print(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-152.8900]) tensor([-1.])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Tanh()\n",
    "output = m(input)\n",
    "print(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-152.8900]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Softmax(dim=0)\n",
    "output = m(input)\n",
    "print(input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point being, activation function choice has a huge impact and as Josh covered, they each have pros and cons. For example, when you have inputs that range from 0-255 (color pixels) you can get away with functions like ReLU that don't care about negative numbers and have better results that differentiate better than things like sigmoid and tanh between 255 and 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
