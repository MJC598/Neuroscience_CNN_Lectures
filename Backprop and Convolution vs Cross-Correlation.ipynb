{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Forward Pass\n",
    "In order to understand backpropagation we need to understand how a foward pass through a neural network works. \n",
    "\n",
    "![Perceptron](perceptron.png)\n",
    "\n",
    "Above is a picture of an individual perceptron. These are the building blocks of neural network layers. On the left in blue circles you see the inputs labelled $x_1$ through $x_n$. If you look closely you will also see a constant 1 value at the top, it represents a bias. We can explore that a bit later, but for right now just know it is there. \n",
    "\n",
    "Next we have the weights in green boxes. There is a weight corresponding to each possible input and the single constant ranging from $w_0$ to $w_n$. These weights are multiplied to each input, think about it as if you're giving an input an importance level.\n",
    "\n",
    "Next is summing together those values. That is represented via the first red circle.\n",
    "\n",
    "Finally, the weighted sum is applied to the non-linearity function (aka hypothesis function, activation function, step-function, etc.). These generally are functions such as the sigmoidal function, $tan(h)$, ReLU, etc. The result of this gives us our output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "For our example we are going to have 4 layers:\n",
    "1. Input Layer (3 values)\n",
    "2. Hidden Layer 1 (4 perceptrons)\n",
    "3. Hidden Layer 2 (4 perceptrons)\n",
    "4. Output Layer (1 value)\n",
    "\n",
    "![Simple Neural Network](simple_neural_network.jpg)\n",
    "\n",
    "We will assume the top value of the input layer is the bias constant 1, and then $x_1 = 0.8$ and $x_2 = 0.5$.\n",
    "\n",
    "Each of the weights throughout the neural network is initialized to be 0.5 to start out with. Keep in mind weight initialization matters in practice because the better the weights are initialized the faster the machine trains.\n",
    "\n",
    "So:\n",
    "\n",
    "$$ws_{1,1} = (C * w_{1,0}) + (x_{1,1} * w_{1,1}) + (x_{1,2} * w_{1,2}) = (1 * 0.5) + (0.8 * 0.5) + (0.5 * 0.5) = 1.15$$\n",
    "\n",
    "Using a sigmoidal activation function:\n",
    "\n",
    "$$output_{1,1} = \\frac{1}{1+e^{-ws_{1,1}}} = \\frac{1}{1+e^{-1.15}} \\approx .735$$\n",
    "\n",
    "Do this same thing for each of the other values in the first Hidden Layer and you get:\n",
    "\n",
    "1. Value 1: .735\n",
    "2. Value 2: .735\n",
    "3. Value 3: .735\n",
    "4. Value 4: .735\n",
    "\n",
    "Notice this is because each of our weights are the same and this is a fully connected network. That means this first pass, each value will be the same. Next, these output values of the first HL become the input values for the second HL, so:\n",
    "\n",
    "$$ws_{2,1} = (x_{2,1} * w_{2,0}) + (x_{2,2} * w_{2,1}) + (x_{2,3} * w_{2,2}) + (x_{2,4} * w_{2,2}) = (.735 * 0.5) + (0.735 * 0.5) + (0.735 * 0.5) + (0.735 * 0.5) = 1.47$$\n",
    "\n",
    "Using a sigmoidal activation function:\n",
    "\n",
    "$$output_{2,1} = \\frac{1}{1+e^{-ws_{2,1}}} = \\frac{1}{1+e^{-1.47}} \\approx .813$$\n",
    "\n",
    "For Hidden Layer 2 you get:\n",
    "\n",
    "1. Value 1: .813\n",
    "2. Value 2: .813\n",
    "3. Value 3: .813\n",
    "4. Value 4: .813\n",
    "\n",
    "Finally, sum them and apply the activation to them for the output:\n",
    "\n",
    "$$ws_{3,1} = (x_{3,1} * w_{3,0}) + (x_{3,2} * w_{3,1}) + (x_{3,3} * w_{3,2}) + (x_{3,4} * w_{3,2}) = (.813 * 0.5) + (0.813 * 0.5) + (0.813 * 0.5) + (0.813 * 0.5) = 1.626$$\n",
    "\n",
    "Using a sigmoidal activation function:\n",
    "\n",
    "$$output_{3,1} = \\frac{1}{1+e^{-ws_{3,1}}} = \\frac{1}{1+e^{-1.626}} \\approx .836$$\n",
    "\n",
    "SO AS A RESULT, $.836$ IS OUR OUTPUT OF 1 PASS THROUGH OUR FEED FORWARD FULLY CONNECTED NEURAL NETWORK!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop Example\n",
    "All we need to do to update the weights, is simply go in reverse! As a reminder:\n",
    "\n",
    "$$\\delta_j(k) = -\\frac{\\partial E(k)}{\\partial e_j(k)}\\frac{\\partial e_j(k)}{\\partial y_j(k)}\\frac{\\partial y_j(k)}{\\partial v_j(k)}$$\n",
    "\n",
    "$$\\delta_j(k) = e_j(k)\\Phi_j'(v_j(k))$$\n",
    "\n",
    "$$\\Delta w_{ij}(k) = \\alpha \\delta_j(k)y_j(k)$$ \n",
    "\n",
    "where $\\Phi'$ is the derivative of the sigmoid activation function so:\n",
    "\n",
    "$$\\Phi' = \\Phi(x) * (1-\\Phi(x))$$ \n",
    "\n",
    "We will assume the label for the output is **.95** and the learning rate **$\\alpha$ = .001**.\n",
    "\n",
    "So starting from the whole output and working towards the weights of Value 1 of Hidden Layer 2, we get the local gradient as:\n",
    "\n",
    "$$\\delta_j(k) = e_j(k)\\Phi_j'(v_j(k)) = (.95 - .836)*(\\Phi(1.626)*(1-\\Phi(1.626))) = .114 * .83562 * .16438 \\approx .016 $$\n",
    "\n",
    "Next we take the output of each value as $y_j(k)$ and start working backwards through the weights. So:\n",
    "\n",
    "$$\\Delta w_{ij}(k) = \\alpha \\delta_j(k)y_j(k) => \\Delta w_{3,1} = \\alpha \\delta_j(k) * output_{2,1} = .001 * .016 * .813 = .000013008$$ \n",
    "\n",
    "Hey, vanishing gradients! But either way, this is the update to the weight 3,1 going from the top perceptron of HL2 to the output layer. Continue this through and calculate the new $\\delta_j(k)$ for each level. When you reach the hidden layer this means you need to recalculate it. This is actually very simple! Its just the summation of the products of the previous deltas and the corresponding weights times the derivative of the activation function. So:\n",
    "\n",
    "$$\\delta_j(k) = \\Phi_j'(v_j(k))\\sum_{1}\\delta_1(k)*w_{1,j}(k)$$\n",
    "\n",
    "\\* **NOTE: This delta update must be done before we update the weights!!!**\n",
    "\\* Also, notational note: The \"1\" index in the preceeding problem goes from output to input, whereas indexing for our weights/inputs goes front to back. So 1 in that equation applied to this network is really 3, then 2, then 1.\n",
    "\n",
    "So:\n",
    "$$\\delta_3(k)\\approx .016$$\n",
    "$$w_{3,1}, w_{3,2}, w_{3,3}, w_{3,4} = .5$$\n",
    "$$\\sum\\delta_3(k)*w_{3,j}(k) = 0.032$$\n",
    "$$\\Phi' = (\\Phi(1.47)*(1-\\Phi(1.47))) = .813 * .187 = .152$$\n",
    "$$\\Phi' * \\sum\\delta_3(k)*w_{3,j}(k) = .152 * .032 \\approx .004864$$\n",
    "\n",
    "And we are good to go with our new local gradient ($\\delta_j(k)$)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU explanation\n",
    "So lets look at our network. If you were carefully watching you probably noticed a lot of the values were simply the sum of products. This, in matrix form, is just a dot product right? So instead of saying:\n",
    "\n",
    "$$\\sum_{1}\\delta_1(k)*w_{1,j}(k)$$\n",
    "\n",
    "We can say:\n",
    "\n",
    "$$\\delta \\cdot w_1$$\n",
    "\n",
    "And avoid looping through each weight! This is a major speed up, especially when considering that these networks can be incredibly deep and involve hundreds if not thousands of weights.\n",
    "\n",
    "Because of their design, GPUs are optimized to handle floating point values where CPUs are not. So, doing dot products of matrices filled with large floating point values is a perfect task for GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "![Perceptron](perceptron.png)\n",
    "\n",
    "\n",
    "### Q1:\n",
    "Apply the forward pass according to the following variables. Assume we are using the sigmoidal activation function. **DON'T FORGET TO INCLUDE THE CONSTANT**\n",
    "\n",
    "$$x_1 = 2$$\n",
    "$$x_2 = 1$$\n",
    "$$x_3 = 3$$\n",
    "$$w_0 = 0.4$$\n",
    "$$w_1 = 0.1$$\n",
    "$$w_2 = -0.6$$\n",
    "$$w_3 = -0.2$$\n",
    "\n",
    "### Q2:\n",
    "Assuming the output result (label) is expected to be $.7$ and your $\\alpha = .1$. What is the final value of the weights after backprop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
